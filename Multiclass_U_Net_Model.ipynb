{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCJpaje6BQhtjZyvQjn0im",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MadmanMarble/Planet_U-NET/blob/main/Multiclass_U_Net_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i09b-ETxbpTx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rasterio"
      ],
      "metadata": {
        "id": "LPlNrlsQcJWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
        "from keras.utils import normalize\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import rasterio\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "YJveoYX3cFRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign image and label variables\n",
        "x_training_01 = sorted(glob.glob(f\"/content/drive/MyDrive/data/images/x_training_140/1year_140/*.tif\"))\n",
        "x_training_02 = sorted(glob.glob(f\"/content/drive/MyDrive/data/images/x_training_140/2year_140/*.tif\"))\n",
        "x_training_03 = sorted(glob.glob(f\"/content/drive/MyDrive/data/images/x_training_140/3year_140/*.tif\"))\n",
        "y_training = sorted(glob.glob(f\"/content/drive/MyDrive/data/labels/y_training_140/*.tif\"))\n",
        "\n",
        "# Split into training and testing datasets (80% train, 20% test)\n",
        "x_train_01, x_test_01, y_train, y_test = train_test_split(x_training_01, y_training, test_size=0.22, random_state=9)\n",
        "x_train_02, x_test_02, y_train, y_test = train_test_split(x_training_02, y_training, test_size=0.22, random_state=9)\n",
        "x_train_03, x_test_03, y_train, y_test = train_test_split(x_training_03, y_training, test_size=0.22, random_state=9)"
      ],
      "metadata": {
        "id": "LQWy27uZcPYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the training and testing dataset to local location\n",
        "\n",
        "# Load the training labels\n",
        "destination = '/content/y_train'\n",
        "os.makedirs(destination, exist_ok=True)\n",
        "for file in y_train:\n",
        "    dest_file = os.path.join(destination, os.path.basename(file))\n",
        "    shutil.copy(file, dest_file)\n",
        "    print(f'Copied {file} to {dest_file}')\n",
        "directory_path = \"/content/y_train\"\n",
        "entries = os.listdir(directory_path)\n",
        "num_files = len(entries)\n",
        "print(f\"There are {num_files} files in {directory_path}.\")\n",
        "\n",
        "# Load the test labels\n",
        "destination = '/content/y_test'\n",
        "os.makedirs(destination, exist_ok=True)\n",
        "for file in y_test:\n",
        "    dest_file = os.path.join(destination, os.path.basename(file))\n",
        "    shutil.copy(file, dest_file)\n",
        "    print(f'Copied {file} to {dest_file}')\n",
        "directory_path = \"/content/y_test\"\n",
        "entries = os.listdir(directory_path)\n",
        "num_files = len(entries)\n",
        "print(f\"There are {num_files} files in {directory_path}.\")\n",
        "\n",
        "\n",
        "# Load the training images for experiment one\n",
        "destination = '/content/x_train_01'\n",
        "os.makedirs(destination, exist_ok=True)\n",
        "for file in x_train_01:\n",
        "    dest_file = os.path.join(destination, os.path.basename(file))\n",
        "    shutil.copy(file, dest_file)\n",
        "    print(f'Copied {file} to {dest_file}')\n",
        "directory_path = \"/content/x_train_01\"\n",
        "entries = os.listdir(directory_path)\n",
        "num_files = len(entries)\n",
        "print(f\"There are {num_files} files in {directory_path}.\")\n",
        "\n",
        "# Load the test images for experiment one\n",
        "destination = '/content/x_test_01'\n",
        "os.makedirs(destination, exist_ok=True)\n",
        "for file in x_test_01:\n",
        "    dest_file = os.path.join(destination, os.path.basename(file))\n",
        "    shutil.copy(file, dest_file)\n",
        "    print(f'Copied {file} to {dest_file}')\n",
        "directory_path = \"/content/x_test_01\"\n",
        "entries = os.listdir(directory_path)\n",
        "num_files = len(entries)\n",
        "print(f\"There are {num_files} files in {directory_path}.\")\n",
        "\n",
        "\n",
        "# Load the training images for experiment two\n",
        "destination = '/content/x_train_02'\n",
        "os.makedirs(destination, exist_ok=True)\n",
        "for file in x_train_02:\n",
        "    dest_file = os.path.join(destination, os.path.basename(file))\n",
        "    shutil.copy(file, dest_file)\n",
        "    print(f'Copied {file} to {dest_file}')\n",
        "directory_path = \"/content/x_train_02\"\n",
        "entries = os.listdir(directory_path)\n",
        "num_files = len(entries)\n",
        "print(f\"There are {num_files} files in {directory_path}.\")\n",
        "\n",
        "# Load the test images for experiment two\n",
        "destination = '/content/x_test_02'\n",
        "os.makedirs(destination, exist_ok=True)\n",
        "for file in x_test_02:\n",
        "    dest_file = os.path.join(destination, os.path.basename(file))\n",
        "    shutil.copy(file, dest_file)\n",
        "    print(f'Copied {file} to {dest_file}')\n",
        "directory_path = \"/content/x_test_02\"\n",
        "entries = os.listdir(directory_path)\n",
        "num_files = len(entries)\n",
        "print(f\"There are {num_files} files in {directory_path}.\")\n",
        "\n",
        "\n",
        "# Load the training images for experiment three\n",
        "destination = '/content/x_train_03'\n",
        "os.makedirs(destination, exist_ok=True)\n",
        "for file in x_train_03:\n",
        "    dest_file = os.path.join(destination, os.path.basename(file))\n",
        "    shutil.copy(file, dest_file)\n",
        "    print(f'Copied {file} to {dest_file}')\n",
        "directory_path = \"/content/x_train_03\"\n",
        "entries = os.listdir(directory_path)\n",
        "num_files = len(entries)\n",
        "print(f\"There are {num_files} files in {directory_path}.\")\n",
        "\n",
        "# Load the test images for experiment three\n",
        "destination = '/content/x_test_03'\n",
        "os.makedirs(destination, exist_ok=True)\n",
        "for file in x_test_03:\n",
        "    dest_file = os.path.join(destination, os.path.basename(file))\n",
        "    shutil.copy(file, dest_file)\n",
        "    print(f'Copied {file} to {dest_file}')\n",
        "directory_path = \"/content/x_test_03\"\n",
        "entries = os.listdir(directory_path)\n",
        "num_files = len(entries)\n",
        "print(f\"There are {num_files} files in {directory_path}.\")"
      ],
      "metadata": {
        "id": "4kHf4DwKcmkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign variables to each dataset set on the local location\n",
        "y_train = sorted(glob.glob(f\"/content/y_train/*.tif\"))\n",
        "y_test= sorted(glob.glob(f\"/content/y_test/*.tif\"))\n",
        "\n",
        "x_train_01 = sorted(glob.glob(f\"/content/x_train_01/*.tif\"))\n",
        "x_test_01 = sorted(glob.glob(f\"/content/x_test_01/*.tif\"))\n",
        "\n",
        "x_train_02 = sorted(glob.glob(f\"/content/x_train_02/*.tif\"))\n",
        "x_test_02 = sorted(glob.glob(f\"/content/x_test_02/*.tif\"))\n",
        "\n",
        "x_train_03 = sorted(glob.glob(f\"/content/x_train_03/*.tif\"))\n",
        "x_test_03 = sorted(glob.glob(f\"/content/x_test_03/*.tif\"))"
      ],
      "metadata": {
        "id": "SFQriOAOg-ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data loader for label data\n",
        "def read_image_labels(file_path):\n",
        "    with rasterio.open(file_path) as src:\n",
        "      label_image = src.read(1)\n",
        "      return label_image.astype(np.int32)"
      ],
      "metadata": {
        "id": "ORY7JUmkhzGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data loader for one year of images\n",
        "def read_multi_image_withfive(file_path):\n",
        "    with rasterio.open(file_path) as src:\n",
        "      bands = [src.read(i) for i in range(1,5)]\n",
        "      image = np.stack(bands, axis=-1)\n",
        "      image = image.astype(float)\n",
        "      max_value = image.max(axis=(0,1), keepdims=True)\n",
        "      min_value = image.min(axis=(0,1), keepdims=True)\n",
        "      image = (image - min_value) / (max_value - min_value)\n",
        "      band_five = src.read(5)\n",
        "      band_five = band_five.astype(float) /band_five.max()\n",
        "      band_five = np.expand_dims(band_five, axis=-1)\n",
        "      combined_image = np.concatenate([image, band_five], axis=-1)\n",
        "      return combined_image"
      ],
      "metadata": {
        "id": "llshrbN9hS_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data loader for two years of images\n",
        "def read_nine_band_image(file_path):\n",
        "    with rasterio.open(file_path) as src:\n",
        "      bands = [src.read(i) for i in range(1,9)]\n",
        "      image = np.stack(bands, axis=-1)\n",
        "      image = image.astype(float)\n",
        "      max_value = image.max(axis=(0,1), keepdims=True)\n",
        "      min_value = image.min(axis=(0,1), keepdims=True)\n",
        "      image = (image - min_value) / (max_value - min_value)\n",
        "      band_five = src.read(9)\n",
        "      band_five = band_five.astype(float) /band_five.max()\n",
        "      band_five = np.expand_dims(band_five, axis=-1)\n",
        "      combined_image = np.concatenate([image, band_five], axis=-1)\n",
        "      return combined_image"
      ],
      "metadata": {
        "id": "XuzXsPH_hk-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data loader for three years of images\n",
        "def read_thirteen_band_image(file_path):\n",
        "    with rasterio.open(file_path) as src:\n",
        "      bands = [src.read(i) for i in range(1,13)]\n",
        "      image = np.stack(bands, axis=-1)\n",
        "      image = image.astype(float)\n",
        "      max_value = image.max(axis=(0,1), keepdims=True)\n",
        "      min_value = image.min(axis=(0,1), keepdims=True)\n",
        "      image = (image - min_value) / (max_value - min_value)\n",
        "      band_five = src.read(13)\n",
        "      band_five = band_five.astype(float) /band_five.max()\n",
        "      band_five = np.expand_dims(band_five, axis=-1)\n",
        "      combined_image = np.concatenate([image, band_five], axis=-1)\n",
        "      return combined_image\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "dz_0FU8ZhtYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign training data and test data variables\n",
        "y_train_data = [read_image_labels(file_path) for file_path in y_train]\n",
        "y_test_data = [read_image_labels(file_path) for file_path in y_test]\n",
        "\n",
        "x_train_01_data = [read_multi_image_withfive(file_path) for file_path in x_train_01]\n",
        "x_test_01_data = [read_multi_image_withfive(file_path) for file_path in x_test_01]\n",
        "\n",
        "x_train_02_data = [read_nine_band_image(file_path) for file_path in x_train_02]\n",
        "x_test_02_data = [read_nine_band_image(file_path) for file_path in x_test_02]\n",
        "\n",
        "x_train_03_data = [read_thirteen_band_image(file_path) for file_path in x_train_03]\n",
        "x_test_03_data = [read_thirteen_band_image(file_path) for file_path in x_test_03]"
      ],
      "metadata": {
        "id": "8-NftyPdh57E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the training data and test data for each experiment from a list to a NumPy array\n",
        "x_train_01 = np.array(x_train_01_data)\n",
        "x_test_01 = np.array(x_test_01_data)\n",
        "\n",
        "x_train_02 = np.array(x_train_02_data)\n",
        "x_test_02 = np.array(x_test_02_data)\n",
        "\n",
        "x_train_03 = np.array(x_train_03_data)\n",
        "x_test_03 = np.array(x_test_03_data)\n",
        "\n",
        "y_train = np.array(y_train_data)\n",
        "y_test = np.array(y_test_data)"
      ],
      "metadata": {
        "id": "EXgeJllviG4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substract 1 from the label training data and label test data so the range starts at 0.\n",
        "# Subtract 1 from each label in the training dataset. This is commonly done to shift\n",
        "# label indices from a range that starts at 1 to a range that starts at 0, which is\n",
        "# standard in many machine learning frameworks that expect zero-indexed labels.\n",
        "y_train_shifted = y_train -1\n",
        "y_test_shifted = y_test -1"
      ],
      "metadata": {
        "id": "1ZUAESwki5i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the shifted training labels into one-hot encoded format. `to_categorical`\n",
        "# function is used to create a binary matrix representation of the labels.\n",
        "# `num_classes=7` indicates that there are seven different classes, ensuring\n",
        "# that the output matrix columns correspond to each class.\n",
        "y_train_encoded = to_categorical(y_train_shifted, num_classes = 7)\n",
        "y_test_encoded = to_categorical(y_test_shifted, num_classes = 7)"
      ],
      "metadata": {
        "id": "jlizZT-Dju2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unet_model(input_size=(384, 384, 13), num_classes=7):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Downsampling (Contracting Path)\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    c1 = BatchNormalization()(c1)\n",
        "    c1 = Dropout(0.1)(c1)\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "    c1 = BatchNormalization()(c1)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "    c2 = BatchNormalization()(c2)\n",
        "    c2 = Dropout(0.1)(c2)\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "    c2 = BatchNormalization()(c2)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    # Bottleneck\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "    c3 = BatchNormalization()(c3)\n",
        "    c3 = Dropout(0.2)(c3)\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "    c3 = BatchNormalization()(c3)\n",
        "\n",
        "    # Upsampling (Expanding Path)\n",
        "    u4 = UpSampling2D((2, 2))(c3)\n",
        "    u4 = concatenate([u4, c2])\n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u4)\n",
        "    c4 = BatchNormalization()(c4)\n",
        "    c4 = Dropout(0.1)(c4)\n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "    c4 = BatchNormalization()(c4)\n",
        "\n",
        "    u5 = UpSampling2D((2, 2))(c4)\n",
        "    u5 = concatenate([u5, c1], axis=3)\n",
        "    c5 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u5)\n",
        "    c5 = BatchNormalization()(c5)\n",
        "    c5 = Dropout(0.1)(c5)\n",
        "    c5 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "    c5 = BatchNormalization()(c5)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(c5)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the U-Net model\n",
        "model = unet_model()\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "rBc4gqXekUmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build callback to save predictions during training.\n",
        "class SavePredictionCallback(Callback):\n",
        "    def __init__(self, x_holdout_dir, output_folder, model_input_size, epoch_frequency=5):\n",
        "        self.x_holdout_dir = glob.glob(x_holdout_dir)\n",
        "        self.output_folder = output_folder\n",
        "        self.model_input_size = model_input_size\n",
        "        self.epoch_frequency = epoch_frequency\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch % self.epoch_frequency == 0:\n",
        "            for image_path in self.x_holdout_dir:\n",
        "                with rasterio.open(image_path) as src:\n",
        "                    bands = [src.read(i) for i in range(1, 13)]\n",
        "                    image = np.stack(bands, axis=-1)\n",
        "                    image = image.astype(float)\n",
        "                    max_value = image.max(axis=(0, 1), keepdims=True)\n",
        "                    min_value = image.min(axis=(0, 1), keepdims=True)\n",
        "                    image = (image - min_value) / (max_value - min_value)\n",
        "                    band_five = src.read(13)\n",
        "                    band_five = band_five.astype(float) / band_five.max()\n",
        "                    band_five = np.expand_dims(band_five, axis=-1)\n",
        "                    combined_image = np.concatenate([image, band_five], axis=-1)\n",
        "                    combined_image_batch = np.expand_dims(combined_image, axis=0)\n",
        "\n",
        "                # Predict using the model\n",
        "                prediction = self.model.predict(combined_image_batch)\n",
        "                prediction_image = np.argmax(prediction, axis=-1)[0, :, :]\n",
        "\n",
        "                # Prepare the output path\n",
        "                output_path = os.path.join(self.output_folder, f\"epoch_{epoch+1}_{os.path.basename(image_path)}\")\n",
        "\n",
        "                # Save the prediction using rasterio\n",
        "                meta = src.meta.copy()\n",
        "                meta.update(dtype=rasterio.uint8, count=1, compress='lzw', nodata= None)\n",
        "                with rasterio.open(output_path, 'w', **meta) as dst:\n",
        "                    dst.write(prediction_image.astype(rasterio.uint8), 1)\n",
        "\n",
        "# Initialization of the callback\n",
        "img_height, img_width = 384, 384\n",
        "output_folder = \"/content/drive/MyDrive/data/model_prediction/threeyear_data/20240418_1448\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "x_holdout_dir = \"/content/drive/MyDrive/data/images/x_test/3year/*.tif\" # Change this based on which experiment is running.\n",
        "\n",
        "savepred_callback = SavePredictionCallback(\n",
        "    x_holdout_dir=x_holdout_dir,\n",
        "    output_folder=output_folder,\n",
        "    model_input_size = (img_height, img_width),\n",
        "    epoch_frequency=5\n",
        ")\n"
      ],
      "metadata": {
        "id": "vgAKd7voeHnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Adam optimizer with a specific learning rate\n",
        "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# Compile the model with the optimizer, loss function, and\n",
        "model.compile(optimizer = adam_optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy', 'categorical_accuracy'])\n",
        "callbacks = [keras.callbacks.ModelCheckpoint(filepath = \"/content/drive/MyDrive/data/model_prediction/threeyear_data/20240418_1448/20240418_1448_3.h5\", monitor = 'val_loss', save_best_only = True), savepred_callback]"
      ],
      "metadata": {
        "id": "KKX6Ky62lSNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model\n",
        "model.fit(three_x_train, y_train_encoded, batch_size=8, epochs=500, validation_data=(three_x_test, y_test_encoded),\n",
        "          callbacks=[callbacks])"
      ],
      "metadata": {
        "id": "1KsdUmbllYyg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}